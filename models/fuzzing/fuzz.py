'''This file is for generating new test cases. (RQ3)'''

import nltk
nltk.download('punkt')
import torch
from transformers import BertTokenizer, BertForMaskedLM
from transformers import AutoTokenizer, AutoModelForTokenClassification
from tqdm import tqdm
from torch.nn import functional as F

class TextDataset(torch.utils.data.Dataset):
    def __init__(self, encodings):
        self.encodings = encodings

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx])
                for key, val in self.encodings.items()}
        return item

    def __len__(self):
        return len(self.encodings["input_ids"])

def sort(texts, fpaths, probabilities):
    list1 = probabilities
    indexs = [i for i in range(len(probabilities))]
    list1, indexs = zip(*sorted(zip(list1, indexs)))

    indexs = list(indexs)[::-1]  # reverse

    texts = list(map(texts.__getitem__, indexs))
    fpaths = list(map(fpaths.__getitem__, indexs))
    probabilities = list(map(probabilities.__getitem__, indexs))

    return texts, fpaths, probabilities

def replace_token_in_text(text, position, substitue):
    '''
    given a text and a position to be replaced, this function replaces the token with the substitue
    '''
    tokens = text.split(" ")
    tokens[position] = substitue
    text = " ".join(tokens)
    return text

def get_nouns_list(text):
    '''
    This function returns a list of nouns in the input text.
    Return a list: [(position, word)]
    '''
    tokens = nltk.word_tokenize(text) # tokenize the text
    tagged = nltk.pos_tag(tokens) # tag the tokens
    # get position of nouns
    nouns = [(i, word) for i, (word, tag) in enumerate(tagged) if tag in ['NN', 'NNP', 'NNS', 'NNPS']]
    return nouns

def get_substitues_from_BERT(text, position, tokenizer, k=20):
    '''
    Given a text and a position to be masked, this function returns a list of substitues generated by the "masked language prediction" function of BERT.
    '''
    text = replace_token_in_text(text, position, tokenizer.mask_token) # replace the token with the mask token

    input = tokenizer.encode(text, return_tensors="pt") # encode the text

    output = mlm_model(input) # get the output of the model
    logits = output.logits # get the logits

    softmax = F.softmax(logits, dim=-1) # get the softmax of the logits
    mask_word = softmax[0, position + 1, :] 
    # Notes: Here we need to add 1 to the position because the first token is '[CLS]'

    # get top 10 substitues
    top_k = torch.topk(mask_word, k)[1]
    substitues = []
    for token in top_k:
        token = token.item()
        substitues.append((tokenizer.decode([token])))
    
    return substitues

def get_err_score_for_one_position(text, mask_position, substitues):
    '''
    given a masked position and k substitues generated by BERT
    return the error score for each substitue (and its surrouding tokens)
    '''
    # get surrouding words
    sub_contexts = {} # {substitute: surrouding words}
    for sub in substitues:
        tokens = text.split(" ")
        context = tokens[mask_position - 1] + " " + sub + " " + tokens[mask_position + 1] # concatenate the surrouding words and the substitute
        sub_contexts[sub] = context # add the context to the dictionary

    substitues = [sub for sub, _ in sub_contexts.items()] # get the substitues
    contexts = [context for _, context in sub_contexts.items()] # get the list of contexts

    text_encodings = predictor_tokenizer(contexts, truncation=True)
    dataset = TextDataset(text_encodings)

    # prepare data loader
    test_loader = torch.utils.data.DataLoader(
            dataset, batch_size=1, shuffle=False)
    res = []
    # score for each test case
    for batch in tqdm(test_loader):
        input_ids = batch['input_ids'].cuda()
        attention_mask = batch['attention_mask'].cuda()
        outputs = predictor_model(input_ids, attention_mask)

        probs = F.softmax(outputs.logits, -1)
        error_probs = probs[:, :, 1]
        res.append(error_probs[0].cpu().detach().numpy().tolist())
    # get the score of the substitue
    scores = [sum(n)/len(n) for n in res]
    # sort the substitues according to the score
    substitues, contexts, scores = sort(substitues, contexts, scores)

    return substitues, contexts, scores

if __name__=='__main__':
    # obtain the list of nouns
    text = "Perhaps she had already met her fate a little deeper in the forest"
    nouns = get_nouns_list(text)
    print(nouns)

    # get substitues from BERT
    model_name = "bert-base-uncased"
    tokenizer = BertTokenizer.from_pretrained(model_name)
    mlm_model = BertForMaskedLM.from_pretrained(model_name)

    # load the word error predictor
    checkpoint = "../error_model/word_error_predictor/EBVS/1/checkpoint-42"
    predictor_tokenizer = AutoTokenizer.from_pretrained(checkpoint)
    predictor_model = AutoModelForTokenClassification.from_pretrained(checkpoint, num_labels=3)
    predictor_model.cuda()
    predictor_model.eval()

    for noun in nouns:
        position = noun[0] # get the position of the noun in the sentence
        if position == len(text.split()) - 1:
            # skip the last word, as typically it will be predicted as ',.?!'
            continue
        word = noun[1] # get the word
        substitues = get_substitues_from_BERT(text, position, tokenizer) # generate substitues from BERT using masked language prediction

        ranked_substitues, contexts, scores = get_err_score_for_one_position(text, position, substitues) # get ranked list of error score for each substitue
        for sub, ctx in zip(ranked_substitues, contexts):
            print(replace_token_in_text(text, position, sub))





