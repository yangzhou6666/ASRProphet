'''This file is for generating new test cases. (RQ3)'''

import nltk
nltk.download('punkt')
import torch
from transformers import BertTokenizer, BertForMaskedLM
from transformers import AutoTokenizer, AutoModelForTokenClassification
from tqdm import tqdm
from torch.nn import functional as F

class TextDataset(torch.utils.data.Dataset):
    def __init__(self, encodings):
        self.encodings = encodings

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx])
                for key, val in self.encodings.items()}
        return item

    def __len__(self):
        return len(self.encodings["input_ids"])

def get_nouns_list(text):
    '''
    This function returns a list of nouns in the input text.
    Return a list: [(position, word)]
    '''
    tokens = nltk.word_tokenize(text) # tokenize the text
    tagged = nltk.pos_tag(tokens) # tag the tokens
    # get position of nouns
    nouns = [(i, word) for i, (word, tag) in enumerate(tagged) if tag in ['NN', 'NNP', 'NNS', 'NNPS']]
    return nouns

def get_substitues_from_BERT(text, position, tokenizer, k=20):
    '''
    Given a text and a position to be masked, this function returns a list of substitues generated by the "masked language prediction" function of BERT.
    '''
    tokens = text.split(" ")
    tokens[position] = tokenizer.mask_token # replace the token with the mask token
    text = " ".join(tokens) # reconstruct the text

    input = tokenizer.encode(text, return_tensors="pt") # encode the text

    output = mlm_model(input) # get the output of the model
    logits = output.logits # get the logits

    softmax = F.softmax(logits, dim=-1) # get the softmax of the logits
    mask_word = softmax[0, position + 1, :] 
    # Notes: Here we need to add 1 to the position because the first token is '[CLS]'

    # get top 10 substitues
    top_k = torch.topk(mask_word, k)[1]
    substitues = []
    for token in top_k:
        token = token.item()
        substitues.append((tokenizer.decode([token])))
    
    return substitues

if __name__=='__main__':
    # obtain the list of nouns
    text = "Perhaps she had already met her fate a little deeper in the forest"
    nouns = get_nouns_list(text)
    print(nouns)

    # get substitues from BERT
    model_name = "bert-base-uncased"
    tokenizer = BertTokenizer.from_pretrained(model_name)
    mlm_model = BertForMaskedLM.from_pretrained(model_name)

    checkpoint = "../error_model/word_error_predictor/EBVS/1/checkpoint-42"
    # load the word error predictor
    predictor_tokenizer = AutoTokenizer.from_pretrained(checkpoint)
    predictor_model = AutoModelForTokenClassification.from_pretrained(checkpoint, num_labels=3)
    predictor_model.cuda()
    predictor_model.eval()

    for noun in nouns:
        position = noun[0]
        if position == len(text.split()) - 1:
            # skip the last word, as typically it will be predicted as ',.?!'
            continue
        word = noun[1]
        print("word:", word)
        substitues = get_substitues_from_BERT(text, position, tokenizer)

        # get surrouding words
        sub_contexts = {}
        for sub in substitues:
            tokens = text.split(" ")
            context = tokens[position - 1] + " " + sub + " " + tokens[position + 1]
            sub_contexts[sub] = context


        for sub, context in sub_contexts.items():
            print("sub:", sub)
            print("context:", context)
            # get the label of the substitue
            test_encodings = predictor_tokenizer([context], truncation=True)
            test_dataset = TextDataset(test_encodings)

            # prepare data loader
            test_loader = torch.utils.data.DataLoader(
                    test_dataset, batch_size=1, shuffle=False)
            res = []
            # score for each test case
            for batch in tqdm(test_loader):
                input_ids = batch['input_ids'].cuda()
                attention_mask = batch['attention_mask'].cuda()
                outputs = predictor_model(input_ids, attention_mask)

                probs = F.softmax(outputs.logits, -1)
                error_probs = probs[:, :, 1]
                res.append(error_probs[0].cpu().detach().numpy().tolist())
            # get the score of the substitue
            print(res)


