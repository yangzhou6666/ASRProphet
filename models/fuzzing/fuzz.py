'''This file is for generating new test cases. (RQ3)'''

import json
import nltk
nltk.download('punkt')
import torch
from transformers import BertTokenizer, BertForMaskedLM
from transformers import AutoTokenizer, AutoModelForTokenClassification
from tqdm import tqdm
from torch.nn import functional as F
import argparse
import os
import re

filter_words = ['.', ',', '?', '!', ':', ';', '"', "'", '-', '_', '(', ')', '[', ']', '{', '}', '#', '@', '$', '%', '^', '&', '*', '+', '=', '<', '>', '/', '\\', '|', '~', '`', '\t', '\n', '\r']

class TextDataset(torch.utils.data.Dataset):
    def __init__(self, encodings):
        self.encodings = encodings

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx])
                for key, val in self.encodings.items()}
        return item

    def __len__(self):
        return len(self.encodings["input_ids"])

def sort(texts, fpaths, probabilities):
    list1 = probabilities
    indexs = [i for i in range(len(probabilities))]
    list1, indexs = zip(*sorted(zip(list1, indexs)))

    indexs = list(indexs)[::-1]  # reverse

    texts = list(map(texts.__getitem__, indexs))
    fpaths = list(map(fpaths.__getitem__, indexs))
    probabilities = list(map(probabilities.__getitem__, indexs))

    return texts, fpaths, probabilities

def replace_token_in_text(text, position, substitue):
    '''
    given a text and a position to be replaced, this function replaces the token with the substitue
    '''
    tokens = text.split(" ")
    tokens[position] = substitue
    text = " ".join(tokens)
    return text

def get_nouns_list(text):
    '''
    This function returns a list of nouns in the input text.
    Return a list: [(position, word)]
    '''
    tokens = nltk.word_tokenize(text) # tokenize the text
    tagged = nltk.pos_tag(tokens) # tag the tokens
    # get position of nouns
    nouns = [(i, word) for i, (word, tag) in enumerate(tagged) if tag in ['NN', 'NNP', 'NNS', 'NNPS']]
    return nouns

def get_substitues_from_BERT(text, position, tokenizer, k=20):
    '''
    Given a text and a position to be masked, this function returns a list of substitues generated by the "masked language prediction" function of BERT.
    '''
    try:
        text = replace_token_in_text(text, position, tokenizer.mask_token) # replace the token with the mask token
    except:
        print(text)
        raise

    input = tokenizer.encode(text, return_tensors="pt") # encode the text

    output = mlm_model(input) # get the output of the model
    logits = output.logits # get the logits

    softmax = F.softmax(logits, dim=-1) # get the softmax of the logits
    mask_word = softmax[0, position + 1, :] 
    # Notes: Here we need to add 1 to the position because the first token is '[CLS]'

    # get top 10 substitues
    top_k = torch.topk(mask_word, k)[1]
    substitues = []
    for token in top_k:
        token = token.item()
        token = tokenizer.decode([token])
        if token in filter_words or '#' in token:
            continue
        substitues.append(token)
    
    return substitues

def get_err_score_for_one_position(text, mask_position, substitues):
    '''
    given a masked position and k substitues generated by BERT
    return the error score for each substitue (and its surrouding tokens)
    '''
    # get surrouding words
    sub_contexts = {} # {substitute: surrouding words}
    for sub in substitues:
        tokens = text.split(" ")
        context = tokens[mask_position - 1] + " " + sub + " " + tokens[mask_position + 1] # concatenate the surrouding words and the substitute
        sub_contexts[sub] = context # add the context to the dictionary

    substitues = [sub for sub, _ in sub_contexts.items()] # get the substitues
    contexts = [context for _, context in sub_contexts.items()] # get the list of contexts

    text_encodings = predictor_tokenizer(contexts, truncation=True)
    dataset = TextDataset(text_encodings)

    # prepare data loader
    test_loader = torch.utils.data.DataLoader(
            dataset, batch_size=1, shuffle=False)
    res = []
    # score for each test case
    for batch in tqdm(test_loader):
        input_ids = batch['input_ids'].cuda()
        attention_mask = batch['attention_mask'].cuda()
        outputs = predictor_model(input_ids, attention_mask)

        probs = F.softmax(outputs.logits, -1)
        error_probs = probs[:, :, 1]
        res.append(error_probs[0].cpu().detach().numpy().tolist())
    # get the score of the substitue
    scores = [sum(n)/len(n) for n in res]
    # sort the substitues according to the score
    substitues, contexts, scores = sort(substitues, contexts, scores)

    return substitues, contexts, scores

def load_text_from_file(fpath):
    correct_texts = {}
    with open(fpath, 'r') as f:
        for chunk in f.read().strip().split('\n\n'):
            data = chunk.split('\n')
            id = data[0]
            WER = float(data[1][5:])
            ref = data[3][5:]
            if WER < 0.001:
                correct_texts[id] = ref
    return correct_texts

def extract_info_from_path(path):
    '''
    given a wav path, this function returns the accent and file id.
    '''
    accent = path.split('/')[2]
    id = path.split('/')[-1].split('.')[0]
    return accent, id

def fuzz(text, wav_path):
    nouns = get_nouns_list(text)
    print(nouns)
    accent, root_id = extract_info_from_path(wav_path)

    for noun in nouns:
        position = noun[0] # get the position of the noun in the sentence
        if position == len(text.split()) - 1:
            # skip the last word, as typically it will be predicted as ',.?!'
            continue
        word = noun[1] # get the word
        substitues = get_substitues_from_BERT(text, position, tokenizer) # generate substitues from BERT using masked language prediction

        ranked_substitues, contexts, scores = get_err_score_for_one_position(text, position, substitues) # get ranked list of error score for each substitue

        # store them in json
        # {text, root_id, sub_id, audio file path, position, original_word, substitues, score}
        for i, sub in enumerate(ranked_substitues):
            new_text = replace_token_in_text(text, position, sub)
            sub_id = root_id + '_' + str(position) + '_' + str(i)
            new_wav_path = wav_path.replace(root_id, sub_id)
            new_wav_path = new_wav_path.replace("/wav/", "/TTS/")
            data = {
                'text': new_text,
                'root_id': root_id,
                'sub_id': sub_id,
                'audio_filepath': new_wav_path,
                'position': position,
                'original_word': word,
                'substitute': sub,
                'score': scores[i],
                'duration': 10.0
            }
            with open(os.path.join(data_folder, 'replacement.json'), 'a') as f:
                f.write(json.dumps(data) + '\n')


def parse_args():
    parser = argparse.ArgumentParser(description='Fuzzing the ASR models')
    parser.add_argument("--seed_json_file", type=str,
                        required=True, help='path to seed json file')
    parser.add_argument("--selection_json_file", type=str,
                        required=True, help='path to input file')
    parser.add_argument("--data_folder", type=str,
                        required=True, help='path to input file')
    parser.add_argument("--finetuned_ckpt", default=None,
                        type=str, help='path to finetuned ckpt')
    parser.add_argument("--seed", default=1, type=int, help='seed id')
    parser.add_argument("--output_json_path", type=str,
                        required=True, help='json fpath to save the ranked texts')
    args=parser.parse_args()
    return args

if __name__=='__main__':
    # get substitues from BERT
    model_name = "bert-base-uncased"
    tokenizer = BertTokenizer.from_pretrained(model_name)
    mlm_model = BertForMaskedLM.from_pretrained(model_name)

    # load the word error predictor
    checkpoint = "/workspace/models/pretrained_checkpoints/word_error_predictor/quartznet/ASI/seed_1/best"
    
    predictor_tokenizer = AutoTokenizer.from_pretrained(checkpoint)
    predictor_model = AutoModelForTokenClassification.from_pretrained(checkpoint, num_labels=3)
    predictor_model.cuda()
    predictor_model.eval()

    # obtain the list of nouns
    data_folder = "/workspace/data/l2arctic/processed/ASI/manifests/"
    if os.path.exists(os.path.join(data_folder, 'replacement.json')):
        os.remove(os.path.join(data_folder, 'replacement.json'))

    transcription_path = "/workspace/data/l2arctic/processed/ASI/manifests/quartznet_outputs/original_test_out.txt"
    wav_text_pairs = load_text_from_file(transcription_path)
    for wav_id, text in wav_text_pairs.items():
        if "'" in text:
            continue
        fuzz(text, wav_id)







