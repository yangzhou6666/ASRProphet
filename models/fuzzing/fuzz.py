'''This file is for generating new test cases. (RQ3)'''

import nltk
nltk.download('punkt')
import torch
from transformers import BertTokenizer, BertForMaskedLM
from transformers import AutoTokenizer, AutoModelForTokenClassification

from torch.nn import functional as F


def get_nouns_list(text):
    '''
    This function returns a list of nouns in the input text.
    Return a list: [(position, word)]
    '''
    tokens = nltk.word_tokenize(text) # tokenize the text
    tagged = nltk.pos_tag(tokens) # tag the tokens
    # get position of nouns
    nouns = [(i, word) for i, (word, tag) in enumerate(tagged) if tag in ['NN', 'NNP', 'NNS', 'NNPS']]
    return nouns

def get_substitues_from_BERT(text, position, tokenizer, k=20):
    '''
    Given a text and a position to be masked, this function returns a list of substitues generated by the "masked language prediction" function of BERT.
    '''
    tokens = text.split(" ")
    tokens[position] = tokenizer.mask_token # replace the token with the mask token
    text = " ".join(tokens) # reconstruct the text

    input = tokenizer.encode(text, return_tensors="pt") # encode the text

    output = mlm_model(input) # get the output of the model
    logits = output.logits # get the logits

    softmax = F.softmax(logits, dim=-1) # get the softmax of the logits
    mask_word = softmax[0, position + 1, :] 
    # Notes: Here we need to add 1 to the position because the first token is '[CLS]'

    # get top 10 substitues
    top_k = torch.topk(mask_word, k)[1]
    substitues = []
    for token in top_k:
        token = token.item()
        substitues.append((tokenizer.decode([token])))
    
    return substitues

if __name__=='__main__':
    # obtain the list of nouns
    text = "Perhaps she had already met her fate a little deeper in the forest"
    nouns = get_nouns_list(text)
    print(nouns)

    # get substitues from BERT
    model_name = "bert-base-uncased"
    tokenizer = BertTokenizer.from_pretrained(model_name)
    mlm_model = BertForMaskedLM.from_pretrained(model_name)

    checkpoint = "../error_model/word_error_predictor/EBVS/1/checkpoint-42"
    # load the word error predictor
    predictor_tokenizer = AutoTokenizer.from_pretrained(checkpoint)
    predictor_model = AutoModelForTokenClassification.from_pretrained(checkpoint, num_labels=3)

    for noun in nouns:
        position = noun[0]
        if position == len(text.split()) - 1:
            # skip the last word, as typically it will be predicted as ',.?!'
            continue
        word = noun[1]
        print("word:", word)
        substitues = get_substitues_from_BERT(text, position, tokenizer)

        # get surrouding words
        sub_contexts = {}
        for sub in substitues:
            tokens = text.split(" ")
            context = tokens[position - 1] + " " + sub + " " + tokens[position + 1]
            sub_contexts[sub] = context


        for sub, context in sub_contexts.items():
            print("sub:", sub)
            print("context:", context)
            # get the label of the substitue
            label = predictor_model(predictor_tokenizer.encode_plus(context, return_tensors="pt"))
            print("label:", label)




