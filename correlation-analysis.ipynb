{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.stats import wilcoxon\n",
    "from scipy.spatial import distance\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "pd.options.display.float_format = '{:,.3f}'.format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "shorten_error_model_name = {\n",
    "    \"random\" : \"RND\",\n",
    "    \"error_model_triphone_rich\" : \"TR\",\n",
    "    \"error_model_pure_diversity\" : \"PD\",\n",
    "    \"error_model_without_diversity_enhancing\" : \"IC-WDE\",\n",
    "    \"error_model\" : \"IC\",\n",
    "    \"asrevolve_error_model_real\" : \"ASR-EV\",\n",
    "    \"word_error_predictor_real/no_word_enhance\" : \"NWE\",\n",
    "    \"word_error_predictor_real/word_enhance\": \"WE\"\n",
    "}\n",
    "\n",
    "shorten_finetuned_model_name = {\n",
    "    \"random\": \"RND\",\n",
    "    \"triphone_rich\": \"TR\",\n",
    "    \"pure_diversity\": \"PD\",\n",
    "    \"icassp_without_diversity_enhancing_real_mix\": \"IC-WDE\",\n",
    "    \"icassp_real_mix\": \"IC\",\n",
    "    \"asrevolve_error_model_real\": \"ASR-EV\",\n",
    "    \"word_error_real_mix/no_word_enhance\": \"NWE\",\n",
    "    \"word_error_real_mix/word_enhance\": \"WE\"\n",
    "}\n",
    "\n",
    "def shorten_em_name(tools):\n",
    "    return [shorten_error_model_name[tool] for tool in tools]\n",
    "\n",
    "def shorten_ft_name(tools) :\n",
    "    return [shorten_finetuned_model_name[tool] for tool in tools]\n",
    "\n",
    "tool_short_names = [\"RND\", \"TR\", \"PD\", \"IC-WDE\", \"IC\", \"ASR-EV\", \"NWE\", \"WE\"]\n",
    "\n",
    "finetuned_model_tool_names = [\"random\", \"triphone_rich\", \"pure_diversity\", \"icassp_without_diversity_enhancing_real_mix\", \"icassp_real_mix\",\n",
    "         \"asrevolve_error_model_real\", \"word_error_real_mix/no_word_enhance\", \"word_error_real_mix/word_enhance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The relative improvement of WER after fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result/RQ2.json', 'r') as f:\n",
    "  data = json.load(f)\n",
    "\n",
    "asrs = [\"quartznet\", \"hubert\", \"wav2vec-base\"]\n",
    "datasets = [\"YBAA\", \"ZHAA\", \"ASI\", \"TNI\", \"NCC\",\n",
    "            \"TXHC\", \"EBVS\", \"ERMS\", \"YDCK\", \"YKWK\", \"THV\", \"TLV\"]\n",
    "tools = [\"random\", \"triphone_rich\", \"pure_diversity\", \"icassp_without_diversity_enhancing_real_mix\", \"icassp_real_mix\",\n",
    "         \"asrevolve_error_model_real\", \"word_error_real_mix/no_word_enhance\", \"word_error_real_mix/word_enhance\"]\n",
    "\n",
    "finetuned_model_performance_on_test_set = {}\n",
    "for asr in asrs:\n",
    "    finetuned_model_performance_on_test_set[asr] = {}\n",
    "    for dataset in datasets:\n",
    "        finetuned_model_performance_on_test_set[asr][dataset] = {}\n",
    "        for tool in tools:\n",
    "            finetuned_model_performance_on_test_set[asr][dataset][shorten_finetuned_model_name[tool]] = pd.read_csv(\n",
    "                data[asr][dataset][tool])\n",
    "\n",
    "# finetuned_model_performance_on_test_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result/original.json', 'r') as f:\n",
    "  original_data = json.load(f)\n",
    "\n",
    "original_model_performance_on_test_set = {}\n",
    "\n",
    "for asr in asrs:\n",
    "    original_model_performance_on_test_set[asr] = {}\n",
    "    for dataset in datasets:\n",
    "        original_model_performance_on_test_set[asr][dataset] = original_data[asr][dataset][\"test\"][\"wer\"]\n",
    "\n",
    "# original_model_performance_on_test_set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_improvement_of_finetuned_model = {}\n",
    "for asr in asrs:\n",
    "    relative_improvement_of_finetuned_model[asr] = {}\n",
    "    for dataset in datasets:\n",
    "        relative_improvement_of_finetuned_model[asr][dataset] = {}\n",
    "        for tool in shorten_ft_name(tools):\n",
    "            relative_improvement_of_finetuned_model[asr][dataset][tool] = {}\n",
    "            for metric in [\"WER_Seed1\", \"WER_Seed2\", \"WER_Seed3\", \"WER_Avg\"]:\n",
    "                relative_improvement_of_finetuned_model[asr][dataset][tool][metric] = (\n",
    "                    (original_model_performance_on_test_set[asr][dataset] - finetuned_model_performance_on_test_set[asr][dataset][tool][metric]) / original_model_performance_on_test_set[asr][dataset]).to_list()\n",
    "\n",
    "# relative_improvement_of_finetuned_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_file(fpath, printed, significant_count, non_significant_count):\n",
    "    os.makedirs(os.path.dirname(fpath), exist_ok=True)\n",
    "    with open(fpath, 'w') as f:\n",
    "        f.write(f\"Significant: \\t\\t{significant_count}\\n\")\n",
    "        f.write(f\"Non-significant: \\t{non_significant_count}\\n\")\n",
    "        f.write(\n",
    "            f\"Total: \\t\\t\\t\\t{significant_count + non_significant_count}\\n\")\n",
    "        f.write(\n",
    "            f\"Percentage: \\t\\t{100*significant_count / (significant_count + non_significant_count):.2f}%\\n\\n\")\n",
    "        for line in printed:\n",
    "            f.write(line)\n",
    "            f.write('\\n')\n",
    "\n",
    "\n",
    "CONFIDENCE_LEVEL = 0.05\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Correlation between the original model's WER on the selected samples and the relative improvement of WER after fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result/RQ1.json', 'r') as f:\n",
    "  rq1_data = json.load(f)\n",
    "\n",
    "\n",
    "original_model_performance_on_selected_samples = {}\n",
    "\n",
    "asrs = [\"quartznet\", \"hubert\", \"wav2vec-base\"]\n",
    "datasets = [\"YBAA\", \"ZHAA\", \"ASI\", \"TNI\", \"NCC\",\n",
    "            \"TXHC\", \"EBVS\", \"ERMS\", \"YDCK\", \"YKWK\", \"THV\", \"TLV\"]\n",
    "tools = [\"random\", \"error_model_triphone_rich\", \"error_model_pure_diversity\", \"error_model_without_diversity_enhancing\", \"error_model\",\n",
    "         \"asrevolve_error_model_real\", \"word_error_predictor_real/no_word_enhance\", \"word_error_predictor_real/word_enhance\"]\n",
    "\n",
    "original_model_performance_on_selected_samples = {}\n",
    "for asr in asrs:\n",
    "    original_model_performance_on_selected_samples[asr] = {}\n",
    "    for dataset in datasets:\n",
    "        original_model_performance_on_selected_samples[asr][dataset] = {}\n",
    "        for tool in tools:\n",
    "            original_model_performance_on_selected_samples[asr][dataset][shorten_error_model_name[tool]] = {}\n",
    "            for metric in [\"WER_Seed1\", \"WER_Seed2\", \"WER_Seed3\", \"WER_Avg\"]:\n",
    "                original_model_performance_on_selected_samples[asr][dataset][shorten_error_model_name[tool]][metric] = pd.read_csv(\n",
    "                  rq1_data[asr][dataset][tool])[metric].tolist()\n",
    "\n",
    "# original_model_performance_on_selected_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## without any grouping mechanism\n",
    "\n",
    "printed = []\n",
    "significant_count = 0\n",
    "non_significant_count = 0\n",
    "\n",
    "\n",
    "arr_1 = []\n",
    "arr_2 = []\n",
    "for dataset in datasets:\n",
    "    for asr in asrs:\n",
    "        for tool_short_name, tool in zip(tool_short_names, tools):\n",
    "          for metric in [\"WER_Seed1\", \"WER_Seed2\", \"WER_Seed3\"]:\n",
    "              arr_1 += original_model_performance_on_selected_samples[asr][dataset][tool_short_name][metric]\n",
    "              arr_2 += relative_improvement_of_finetuned_model[asr][dataset][tool_short_name][metric]\n",
    "\n",
    "assert len(arr_1) == len(arr_2)\n",
    "correlation_coefficient, p_value = scipy.stats.spearmanr(arr_1, arr_2)\n",
    "\n",
    "\n",
    "printed.append(f\"Corr coef:\\t{correlation_coefficient}\")\n",
    "printed.append(f\"p-value:\\t{p_value}\")\n",
    "if p_value < CONFIDENCE_LEVEL:\n",
    "    printed.append(f\"SIGNIFICANT\")\n",
    "    significant_count += 1\n",
    "\n",
    "else:\n",
    "    non_significant_count += 1\n",
    "    printed.append(f\"NO\")\n",
    "\n",
    "\n",
    "fpath = \"result/analyze/1/original_model_performance_correlation_without_any_grouping.txt\"\n",
    "save_to_file(fpath, printed, significant_count, non_significant_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## group by dataset\n",
    "\n",
    "printed = []\n",
    "significant_count = 0\n",
    "non_significant_count = 0\n",
    "\n",
    "for dataset in datasets:\n",
    "    arr_1 = []\n",
    "    arr_2 = []\n",
    "    for asr in asrs:\n",
    "        for tool_short_name, tool in zip(tool_short_names, tools):\n",
    "          for metric in [\"WER_Seed1\", \"WER_Seed2\", \"WER_Seed3\"]:\n",
    "              arr_1 += original_model_performance_on_selected_samples[asr][dataset][tool_short_name][metric]\n",
    "              arr_2 += relative_improvement_of_finetuned_model[asr][dataset][tool_short_name][metric]\n",
    "\n",
    "    assert len(arr_1) == len(arr_2)\n",
    "    correlation_coefficient, p_value = scipy.stats.spearmanr(arr_1, arr_2)\n",
    "\n",
    "    printed.append(f\"Dataset: {dataset}\")\n",
    "    printed.append(f\"Corr coef:\\t{correlation_coefficient}\")\n",
    "    printed.append(f\"p-value:\\t{p_value}\")\n",
    "    if p_value < CONFIDENCE_LEVEL:\n",
    "        printed.append(f\"SIGNIFICANT\")\n",
    "        significant_count += 1\n",
    "    else:\n",
    "        printed.append(f\"NO\")\n",
    "        non_significant_count += 1\n",
    "    printed.append(\"\")\n",
    "\n",
    "\n",
    "fpath = \"result/analyze/1/original_model_performance_correlation_group_by_dataset.txt\"\n",
    "save_to_file(fpath, printed, significant_count, non_significant_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## group by dataset and ASR\n",
    "\n",
    "printed = []\n",
    "significant_count = 0\n",
    "non_significant_count = 0\n",
    "\n",
    "\n",
    "for dataset in datasets:\n",
    "    for asr in asrs:\n",
    "        arr_1 = []\n",
    "        arr_2 = []\n",
    "        for tool_short_name, tool in zip(tool_short_names, tools):\n",
    "            for metric in [\"WER_Seed1\", \"WER_Seed2\", \"WER_Seed3\"]:\n",
    "                arr_1 += original_model_performance_on_selected_samples[asr][dataset][tool_short_name][metric]\n",
    "                arr_2 += relative_improvement_of_finetuned_model[asr][dataset][tool_short_name][metric]\n",
    "\n",
    "        assert len(arr_1) == len(arr_2)\n",
    "        correlation_coefficient, p_value = scipy.stats.spearmanr(arr_1, arr_2)\n",
    "\n",
    "        printed.append(f\"Dataset: {dataset}\")\n",
    "        printed.append(f\"ASR: {asr}\")\n",
    "        printed.append(f\"Corr coef:\\t{correlation_coefficient}\")\n",
    "        printed.append(f\"p-value:\\t{p_value}\")\n",
    "        if p_value < CONFIDENCE_LEVEL:\n",
    "            printed.append(f\"SIGNIFICANT\")\n",
    "            significant_count += 1\n",
    "\n",
    "        else:\n",
    "            printed.append(f\"NO\")\n",
    "            non_significant_count += 1\n",
    "        printed.append(\"\")\n",
    "\n",
    "\n",
    "fpath = \"result/analyze/1/original_model_performance_correlation_group_by_dataset_and_asr.txt\"\n",
    "save_to_file(fpath, printed, significant_count, non_significant_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## group by dataset and ASR then averaging budget\n",
    "\n",
    "printed = []\n",
    "significant_count = 0\n",
    "non_significant_count = 0\n",
    "\n",
    "\n",
    "for dataset in datasets:\n",
    "    for asr in asrs:\n",
    "        arr_1 = []\n",
    "        arr_2 = []\n",
    "        for tool_short_name, tool in zip(tool_short_names, tools):\n",
    "            for metric in [\"WER_Seed1\", \"WER_Seed2\", \"WER_Seed3\"]:\n",
    "                curr_arr_1 = original_model_performance_on_selected_samples[asr][dataset][tool_short_name][metric]\n",
    "                arr_1.append(np.mean(curr_arr_1))\n",
    "                curr_arr_2 = relative_improvement_of_finetuned_model[asr][dataset][tool_short_name][metric]\n",
    "                arr_2.append(np.mean(curr_arr_2))\n",
    "\n",
    "\n",
    "        assert len(arr_1) == len(arr_2)\n",
    "        correlation_coefficient, p_value = scipy.stats.spearmanr(arr_1, arr_2)\n",
    "\n",
    "        printed.append(f\"Dataset: {dataset}\")\n",
    "        printed.append(f\"ASR: {asr}\")\n",
    "        printed.append(f\"Corr coef:\\t{correlation_coefficient}\")\n",
    "        printed.append(f\"p-value:\\t{p_value}\")\n",
    "        if p_value < CONFIDENCE_LEVEL:\n",
    "            printed.append(f\"SIGNIFICANT\")\n",
    "            significant_count += 1\n",
    "\n",
    "        else:\n",
    "            printed.append(f\"NO\")\n",
    "            non_significant_count += 1\n",
    "        printed.append(\"\")\n",
    "\n",
    "\n",
    "fpath = \"result/analyze/1/original_model_performance_correlation_group_by_dataset_and_asr_then_averaging_budget.txt\"\n",
    "save_to_file(fpath, printed, significant_count, non_significant_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'random'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-ed78955f5abf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtool_short_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtool\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"WER_Seed1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"WER_Seed2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"WER_Seed3\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mbudget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_model_performance_on_selected_samples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0masr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mbudget\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                         \u001b[0mtemp_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbudget\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'random'"
     ]
    }
   ],
   "source": [
    "## group by dataset and ASR and budget\n",
    "\n",
    "printed = []\n",
    "significant_count = 0\n",
    "non_significant_count = 0\n",
    "\n",
    "\n",
    "for dataset in datasets:\n",
    "    for asr in asrs:\n",
    "        \n",
    "        temp_1 = {}\n",
    "        temp_2 = {}\n",
    "        \n",
    "        \n",
    "        for tool_short_name, tool in zip(tools, tools):\n",
    "            for metric in [\"WER_Seed1\", \"WER_Seed2\", \"WER_Seed3\"]:\n",
    "                for budget, val in zip([100, 200, 300, 400], original_model_performance_on_selected_samples[asr][dataset][tool][metric]):\n",
    "                    if budget not in temp_1:\n",
    "                        temp_1[budget] = []\n",
    "                    temp_1[budget].append(val)\n",
    "\n",
    "                \n",
    "                for budget, val in zip([100, 200, 300, 400], relative_improvement_of_finetuned_model[\n",
    "                    asr][dataset][tool_short_name][metric]) :\n",
    "                    if budget not in temp_2:\n",
    "                        temp_2[budget] = []\n",
    "                    temp_2[budget].append(val)\n",
    "        \n",
    "\n",
    "        for budget in [100, 200, 300, 400]:\n",
    "            arr_1 = temp_1[budget]\n",
    "            arr_2 = temp_2[budget]\n",
    "\n",
    "            assert len(arr_1) == len(arr_2)\n",
    "            correlation_coefficient, p_value = scipy.stats.spearmanr(arr_1, arr_2)\n",
    "\n",
    "            printed.append(f\"Dataset: {dataset}\")\n",
    "            printed.append(f\"ASR: {asr}\")\n",
    "            printed.append(f\"Corr coef:\\t{correlation_coefficient}\")\n",
    "            printed.append(f\"p-value:\\t{p_value}\")\n",
    "            if p_value < CONFIDENCE_LEVEL:\n",
    "                printed.append(f\"SIGNIFICANT\")\n",
    "                significant_count += 1\n",
    "\n",
    "            else:\n",
    "                printed.append(f\"NO\")\n",
    "                non_significant_count += 1\n",
    "            printed.append(\"\")\n",
    "\n",
    "\n",
    "fpath = \"result/analyze/1/original_model_performance_correlation_group_by_dataset_and_asr_and_budget.txt\"\n",
    "save_to_file(fpath, printed, significant_count, non_significant_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Correlation between \"the distance of triphone rich distribution and ideal distribution on the selected samples\" and the relative improvement of WER after fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result/triphone_rich.json', 'r') as f:\n",
    "  triphone_rich = json.load(f)\n",
    "\n",
    "tools = [\"random\", \"error_model_triphone_rich\", \"error_model_pure_diversity\", \"error_model_without_diversity_enhancing\", \"error_model\",\n",
    "         \"asrevolve_error_model_real\", \"word_error_predictor_real/no_word_enhance\", \"word_error_predictor_real/word_enhance\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## without any grouping mechanism\n",
    "\n",
    "printed = []\n",
    "significant_count = 0\n",
    "non_significant_count = 0\n",
    "\n",
    "\n",
    "arr_1 = []\n",
    "arr_2 = []\n",
    "for dataset in datasets:\n",
    "    for asr in asrs:\n",
    "        for tool_short_name, tool in zip(tool_short_names, tools):\n",
    "          for seed in [\"1\", \"2\", \"3\"]:\n",
    "              arr_1 += triphone_rich[asr][dataset][tool][seed]\n",
    "          for metric in [\"WER_Seed1\", \"WER_Seed2\", \"WER_Seed3\"]:\n",
    "              arr_2 += relative_improvement_of_finetuned_model[asr][dataset][tool_short_name][metric]\n",
    "\n",
    "assert len(arr_1) == len(arr_2)\n",
    "correlation_coefficient, p_value = scipy.stats.spearmanr(arr_1, arr_2)\n",
    "\n",
    "\n",
    "printed.append(f\"Corr coef:\\t{correlation_coefficient}\")\n",
    "printed.append(f\"p-value:\\t{p_value}\")\n",
    "if p_value < CONFIDENCE_LEVEL:\n",
    "    printed.append(f\"SIGNIFICANT\")\n",
    "    significant_count += 1\n",
    "\n",
    "else:\n",
    "    non_significant_count += 1\n",
    "    printed.append(f\"NO\")\n",
    "\n",
    "\n",
    "fpath = \"result/analyze/3/triphone_rich_correlation_without_any_grouping.txt\"\n",
    "save_to_file(fpath, printed, significant_count, non_significant_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## group by dataset\n",
    "\n",
    "printed = []\n",
    "significant_count = 0\n",
    "non_significant_count = 0\n",
    "\n",
    "for dataset in datasets:\n",
    "    arr_1 = []\n",
    "    arr_2 = []\n",
    "    for asr in asrs:\n",
    "        for tool_short_name, tool in zip(tool_short_names, tools):\n",
    "          for seed in [\"1\", \"2\", \"3\"]:\n",
    "              arr_1 += triphone_rich[asr][dataset][tool][seed]\n",
    "          for metric in [\"WER_Seed1\", \"WER_Seed2\", \"WER_Seed3\"]:\n",
    "              arr_2 += relative_improvement_of_finetuned_model[asr][dataset][tool_short_name][metric]\n",
    "\n",
    "    assert len(arr_1) == len(arr_2)\n",
    "    correlation_coefficient, p_value = scipy.stats.spearmanr(arr_1, arr_2)\n",
    "\n",
    "    printed.append(f\"Dataset: {dataset}\")\n",
    "    printed.append(f\"Corr coef:\\t{correlation_coefficient}\")\n",
    "    printed.append(f\"p-value:\\t{p_value}\")\n",
    "    if p_value < CONFIDENCE_LEVEL:\n",
    "        printed.append(f\"SIGNIFICANT\")\n",
    "        significant_count += 1\n",
    "    else:\n",
    "        printed.append(f\"NO\")\n",
    "        non_significant_count += 1\n",
    "    printed.append(\"\")\n",
    "\n",
    "\n",
    "fpath = \"result/analyze/3/triphone_rich_correlation_group_by_dataset.txt\"\n",
    "save_to_file(fpath, printed, significant_count, non_significant_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## group by dataset and ASR\n",
    "\n",
    "printed = []\n",
    "significant_count = 0\n",
    "non_significant_count = 0\n",
    "\n",
    "\n",
    "for dataset in datasets:\n",
    "    for asr in asrs:\n",
    "        arr_1 = []\n",
    "        arr_2 = []\n",
    "        for tool_short_name, tool in zip(tool_short_names, tools):\n",
    "          for seed in [\"1\", \"2\", \"3\"]:\n",
    "              arr_1 += triphone_rich[asr][dataset][tool][seed]\n",
    "          for metric in [\"WER_Seed1\", \"WER_Seed2\", \"WER_Seed3\"]:\n",
    "              arr_2 += relative_improvement_of_finetuned_model[asr][dataset][tool_short_name][metric]\n",
    "\n",
    "        assert len(arr_1) == len(arr_2)\n",
    "        correlation_coefficient, p_value = scipy.stats.spearmanr(arr_1, arr_2)\n",
    "\n",
    "        printed.append(f\"Dataset: {dataset}\")\n",
    "        printed.append(f\"ASR: {asr}\")\n",
    "        printed.append(f\"Corr coef:\\t{correlation_coefficient}\")\n",
    "        printed.append(f\"p-value:\\t{p_value}\")\n",
    "        if p_value < CONFIDENCE_LEVEL:\n",
    "            printed.append(f\"SIGNIFICANT\")\n",
    "            significant_count += 1\n",
    "\n",
    "        else:\n",
    "            printed.append(f\"NO\")\n",
    "            non_significant_count += 1\n",
    "        printed.append(\"\")\n",
    "\n",
    "\n",
    "fpath = \"result/analyze/3/triphone_rich_correlation_group_by_dataset_and_asr.txt\"\n",
    "save_to_file(fpath, printed, significant_count, non_significant_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## group by dataset and ASR then averaging budget\n",
    "\n",
    "printed = []\n",
    "significant_count = 0\n",
    "non_significant_count = 0\n",
    "\n",
    "\n",
    "for dataset in datasets:\n",
    "    for asr in asrs:\n",
    "        arr_1 = []\n",
    "        arr_2 = []\n",
    "        for tool_short_name, tool in zip(tool_short_names, tools):\n",
    "            for seed in [\"1\", \"2\", \"3\"]:\n",
    "                curr_arr_1 = triphone_rich[asr][dataset][tool][seed]\n",
    "                arr_1.append(np.mean(curr_arr_1))\n",
    "            \n",
    "            for metric in [\"WER_Seed1\", \"WER_Seed2\", \"WER_Seed3\"]:\n",
    "                curr_arr_2 = relative_improvement_of_finetuned_model[asr][dataset][tool_short_name][metric]\n",
    "                arr_2.append(np.mean(curr_arr_2))\n",
    "\n",
    "\n",
    "        assert len(arr_1) == len(arr_2)\n",
    "        correlation_coefficient, p_value = scipy.stats.spearmanr(arr_1, arr_2)\n",
    "\n",
    "        printed.append(f\"Dataset: {dataset}\")\n",
    "        printed.append(f\"ASR: {asr}\")\n",
    "        printed.append(f\"Corr coef:\\t{correlation_coefficient}\")\n",
    "        printed.append(f\"p-value:\\t{p_value}\")\n",
    "        if p_value < CONFIDENCE_LEVEL:\n",
    "            printed.append(f\"SIGNIFICANT\")\n",
    "            significant_count += 1\n",
    "\n",
    "        else:\n",
    "            printed.append(f\"NO\")\n",
    "            non_significant_count += 1\n",
    "        printed.append(\"\")\n",
    "\n",
    "\n",
    "fpath = \"result/analyze/3/triphone_rich_correlation_group_by_dataset_and_asr_then_averaging_budget.txt\"\n",
    "save_to_file(fpath, printed, significant_count, non_significant_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## group by dataset and ASR and budget\n",
    "\n",
    "printed = []\n",
    "significant_count = 0\n",
    "non_significant_count = 0\n",
    "\n",
    "\n",
    "for dataset in datasets:\n",
    "    for asr in asrs:\n",
    "        \n",
    "        temp_1 = {}\n",
    "        temp_2 = {}\n",
    "        \n",
    "        \n",
    "        for tool_short_name, tool in zip(tool_short_names, tools):\n",
    "            for seed in [\"1\", \"2\", \"3\"]:\n",
    "                for budget, val in zip([100, 200, 300, 400], triphone_rich[asr][dataset][tool][seed]) :\n",
    "                    if budget not in temp_1:\n",
    "                        temp_1[budget] = []\n",
    "                    temp_1[budget].append(val)\n",
    "                \n",
    "\n",
    "            for metric in [\"WER_Seed1\", \"WER_Seed2\", \"WER_Seed3\"]:\n",
    "                for budget, val in zip([100, 200, 300, 400], relative_improvement_of_finetuned_model[\n",
    "                    asr][dataset][tool_short_name][metric]) :\n",
    "                    if budget not in temp_2:\n",
    "                        temp_2[budget] = []\n",
    "                    temp_2[budget].append(val)\n",
    "        \n",
    "\n",
    "        for budget in [100, 200, 300, 400]:\n",
    "            arr_1 = temp_1[budget]\n",
    "            arr_2 = temp_2[budget]\n",
    "\n",
    "            assert len(arr_1) == len(arr_2)\n",
    "            correlation_coefficient, p_value = scipy.stats.spearmanr(arr_1, arr_2)\n",
    "\n",
    "            printed.append(f\"Dataset: {dataset}\")\n",
    "            printed.append(f\"ASR: {asr}\")\n",
    "            printed.append(f\"Corr coef:\\t{correlation_coefficient}\")\n",
    "            printed.append(f\"p-value:\\t{p_value}\")\n",
    "            if p_value < CONFIDENCE_LEVEL:\n",
    "                printed.append(f\"SIGNIFICANT\")\n",
    "                significant_count += 1\n",
    "\n",
    "            else:\n",
    "                printed.append(f\"NO\")\n",
    "                non_significant_count += 1\n",
    "            printed.append(\"\")\n",
    "\n",
    "\n",
    "fpath = \"result/analyze/3/triphone_rich_correlation_group_by_dataset_and_asr_and_budget.txt\"\n",
    "save_to_file(fpath, printed, significant_count, non_significant_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Correlation between the phoneme submodular function on the selected samples and the relative improvement of WER after fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result/phoneme_submodular_function.json', 'r') as f:\n",
    "  phoneme_submodular_function = json.load(f)\n",
    "\n",
    "tools = [\"random\", \"error_model_triphone_rich\", \"error_model_pure_diversity\", \"error_model_without_diversity_enhancing\", \"error_model\",\n",
    "         \"asrevolve_error_model_real\", \"word_error_predictor_real/no_word_enhance\", \"word_error_predictor_real/word_enhance\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## without any grouping mechanism\n",
    "\n",
    "printed = []\n",
    "significant_count = 0\n",
    "non_significant_count = 0\n",
    "\n",
    "\n",
    "arr_1 = []\n",
    "arr_2 = []\n",
    "for dataset in datasets:\n",
    "    for asr in asrs:\n",
    "        for tool_short_name, tool in zip(tool_short_names, tools):\n",
    "          for seed in [\"1\", \"2\", \"3\"]:\n",
    "              arr_1 += phoneme_submodular_function[asr][dataset][tool][seed]\n",
    "          for metric in [\"WER_Seed1\", \"WER_Seed2\", \"WER_Seed3\"]:\n",
    "              arr_2 += relative_improvement_of_finetuned_model[asr][dataset][tool_short_name][metric]\n",
    "\n",
    "assert len(arr_1) == len(arr_2)\n",
    "correlation_coefficient, p_value = scipy.stats.spearmanr(arr_1, arr_2)\n",
    "\n",
    "\n",
    "printed.append(f\"Corr coef:\\t{correlation_coefficient}\")\n",
    "printed.append(f\"p-value:\\t{p_value}\")\n",
    "if p_value < CONFIDENCE_LEVEL:\n",
    "    printed.append(f\"SIGNIFICANT\")\n",
    "    significant_count += 1\n",
    "\n",
    "else:\n",
    "    non_significant_count += 1\n",
    "    printed.append(f\"NO\")\n",
    "\n",
    "\n",
    "fpath = \"result/analyze/4/phoneme_submodular_function_correlation_without_any_grouping.txt\"\n",
    "save_to_file(fpath, printed, significant_count, non_significant_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## group by dataset\n",
    "\n",
    "printed = []\n",
    "significant_count = 0\n",
    "non_significant_count = 0\n",
    "\n",
    "for dataset in datasets:\n",
    "    arr_1 = []\n",
    "    arr_2 = []\n",
    "    for asr in asrs:\n",
    "        for tool_short_name, tool in zip(tool_short_names, tools):\n",
    "          for seed in [\"1\", \"2\", \"3\"]:\n",
    "              arr_1 += phoneme_submodular_function[asr][dataset][tool][seed]\n",
    "          for metric in [\"WER_Seed1\", \"WER_Seed2\", \"WER_Seed3\"]:\n",
    "              arr_2 += relative_improvement_of_finetuned_model[asr][dataset][tool_short_name][metric]\n",
    "\n",
    "    assert len(arr_1) == len(arr_2)\n",
    "    correlation_coefficient, p_value = scipy.stats.spearmanr(arr_1, arr_2)\n",
    "\n",
    "    printed.append(f\"Dataset: {dataset}\")\n",
    "    printed.append(f\"Corr coef:\\t{correlation_coefficient}\")\n",
    "    printed.append(f\"p-value:\\t{p_value}\")\n",
    "    if p_value < CONFIDENCE_LEVEL:\n",
    "        printed.append(f\"SIGNIFICANT\")\n",
    "        significant_count += 1\n",
    "    else:\n",
    "        printed.append(f\"NO\")\n",
    "        non_significant_count += 1\n",
    "    printed.append(\"\")\n",
    "\n",
    "\n",
    "fpath = \"result/analyze/4/phoneme_submodular_function_correlation_group_by_dataset.txt\"\n",
    "save_to_file(fpath, printed, significant_count, non_significant_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## group by dataset and ASR\n",
    "\n",
    "printed = []\n",
    "significant_count = 0\n",
    "non_significant_count = 0\n",
    "\n",
    "\n",
    "for dataset in datasets:\n",
    "    for asr in asrs:\n",
    "        arr_1 = []\n",
    "        arr_2 = []\n",
    "        for tool_short_name, tool in zip(tool_short_names, tools):\n",
    "          for seed in [\"1\", \"2\", \"3\"]:\n",
    "              arr_1 += phoneme_submodular_function[asr][dataset][tool][seed]\n",
    "          for metric in [\"WER_Seed1\", \"WER_Seed2\", \"WER_Seed3\"]:\n",
    "              arr_2 += relative_improvement_of_finetuned_model[asr][dataset][tool_short_name][metric]\n",
    "\n",
    "        assert len(arr_1) == len(arr_2)\n",
    "        correlation_coefficient, p_value = scipy.stats.spearmanr(arr_1, arr_2)\n",
    "\n",
    "        printed.append(f\"Dataset: {dataset}\")\n",
    "        printed.append(f\"ASR: {asr}\")\n",
    "        printed.append(f\"Corr coef:\\t{correlation_coefficient}\")\n",
    "        printed.append(f\"p-value:\\t{p_value}\")\n",
    "        if p_value < CONFIDENCE_LEVEL:\n",
    "            printed.append(f\"SIGNIFICANT\")\n",
    "            significant_count += 1\n",
    "\n",
    "        else:\n",
    "            printed.append(f\"NO\")\n",
    "            non_significant_count += 1\n",
    "        printed.append(\"\")\n",
    "\n",
    "\n",
    "fpath = \"result/analyze/4/phoneme_submodular_function_correlation_group_by_dataset_and_asr.txt\"\n",
    "save_to_file(fpath, printed, significant_count, non_significant_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## group by dataset and ASR then averaging budget\n",
    "\n",
    "printed = []\n",
    "significant_count = 0\n",
    "non_significant_count = 0\n",
    "\n",
    "\n",
    "for dataset in datasets:\n",
    "    for asr in asrs:\n",
    "        arr_1 = []\n",
    "        arr_2 = []\n",
    "        for tool_short_name, tool in zip(tool_short_names, tools):\n",
    "            for seed in [\"1\", \"2\", \"3\"]:\n",
    "                curr_arr_1 = phoneme_submodular_function[asr][dataset][tool][seed]\n",
    "                arr_1.append(np.mean(curr_arr_1))\n",
    "            \n",
    "            for metric in [\"WER_Seed1\", \"WER_Seed2\", \"WER_Seed3\"]:\n",
    "                curr_arr_2 = relative_improvement_of_finetuned_model[asr][dataset][tool_short_name][metric]\n",
    "                arr_2.append(np.mean(curr_arr_2))\n",
    "\n",
    "\n",
    "        assert len(arr_1) == len(arr_2)\n",
    "        correlation_coefficient, p_value = scipy.stats.spearmanr(arr_1, arr_2)\n",
    "\n",
    "        printed.append(f\"Dataset: {dataset}\")\n",
    "        printed.append(f\"ASR: {asr}\")\n",
    "        printed.append(f\"Corr coef:\\t{correlation_coefficient}\")\n",
    "        printed.append(f\"p-value:\\t{p_value}\")\n",
    "        if p_value < CONFIDENCE_LEVEL:\n",
    "            printed.append(f\"SIGNIFICANT\")\n",
    "            significant_count += 1\n",
    "\n",
    "        else:\n",
    "            printed.append(f\"NO\")\n",
    "            non_significant_count += 1\n",
    "        printed.append(\"\")\n",
    "\n",
    "\n",
    "fpath = \"result/analyze/4/phoneme_submodular_function_correlation_group_by_dataset_and_asr_then_averaging_budget.txt\"\n",
    "save_to_file(fpath, printed, significant_count, non_significant_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## group by dataset and ASR and budget\n",
    "\n",
    "printed = []\n",
    "significant_count = 0\n",
    "non_significant_count = 0\n",
    "\n",
    "\n",
    "for dataset in datasets:\n",
    "    for asr in asrs:\n",
    "        \n",
    "        temp_1 = {}\n",
    "        temp_2 = {}\n",
    "        \n",
    "        \n",
    "        for tool_short_name, tool in zip(tool_short_names, tools):\n",
    "            for seed in [\"1\", \"2\", \"3\"]:\n",
    "                for budget, val in zip([100, 200, 300, 400], phoneme_submodular_function[asr][dataset][tool][seed]) :\n",
    "                    if budget not in temp_1:\n",
    "                        temp_1[budget] = []\n",
    "                    temp_1[budget].append(val)\n",
    "                \n",
    "\n",
    "            for metric in [\"WER_Seed1\", \"WER_Seed2\", \"WER_Seed3\"]:\n",
    "                for budget, val in zip([100, 200, 300, 400], relative_improvement_of_finetuned_model[\n",
    "                    asr][dataset][tool_short_name][metric]) :\n",
    "                    if budget not in temp_2:\n",
    "                        temp_2[budget] = []\n",
    "                    temp_2[budget].append(val)\n",
    "        \n",
    "\n",
    "        for budget in [100, 200, 300, 400]:\n",
    "            arr_1 = temp_1[budget]\n",
    "            arr_2 = temp_2[budget]\n",
    "\n",
    "            assert len(arr_1) == len(arr_2)\n",
    "            correlation_coefficient, p_value = scipy.stats.spearmanr(arr_1, arr_2)\n",
    "\n",
    "            printed.append(f\"Dataset: {dataset}\")\n",
    "            printed.append(f\"ASR: {asr}\")\n",
    "            printed.append(f\"Corr coef:\\t{correlation_coefficient}\")\n",
    "            printed.append(f\"p-value:\\t{p_value}\")\n",
    "            if p_value < CONFIDENCE_LEVEL:\n",
    "                printed.append(f\"SIGNIFICANT\")\n",
    "                significant_count += 1\n",
    "\n",
    "            else:\n",
    "                printed.append(f\"NO\")\n",
    "                non_significant_count += 1\n",
    "            printed.append(\"\")\n",
    "\n",
    "\n",
    "fpath = \"result/analyze/4/phoneme_submodular_function_correlation_group_by_dataset_and_asr_and_budget.txt\"\n",
    "save_to_file(fpath, printed, significant_count, non_significant_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Correlation between the number of test cases on the selected samples and the relative improvement of WER after fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result/number_of_test_cases.json', 'r') as f:\n",
    "  number_of_test_cases = json.load(f)\n",
    "\n",
    "tools = [\"random\", \"error_model_triphone_rich\", \"error_model_pure_diversity\", \"error_model_without_diversity_enhancing\", \"error_model\",\n",
    "         \"asrevolve_error_model_real\", \"word_error_predictor_real/no_word_enhance\", \"word_error_predictor_real/word_enhance\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## without any grouping mechanism\n",
    "\n",
    "printed = []\n",
    "significant_count = 0\n",
    "non_significant_count = 0\n",
    "\n",
    "\n",
    "arr_1 = []\n",
    "arr_2 = []\n",
    "for dataset in datasets:\n",
    "    for asr in asrs:\n",
    "        for tool_short_name, tool in zip(tool_short_names, tools):\n",
    "          for seed in [\"1\", \"2\", \"3\"]:\n",
    "              arr_1 += number_of_test_cases[asr][dataset][tool][seed]\n",
    "          for metric in [\"WER_Seed1\", \"WER_Seed2\", \"WER_Seed3\"]:\n",
    "              arr_2 += relative_improvement_of_finetuned_model[asr][dataset][tool_short_name][metric]\n",
    "\n",
    "assert len(arr_1) == len(arr_2)\n",
    "correlation_coefficient, p_value = scipy.stats.spearmanr(arr_1, arr_2)\n",
    "\n",
    "\n",
    "printed.append(f\"Corr coef:\\t{correlation_coefficient}\")\n",
    "printed.append(f\"p-value:\\t{p_value}\")\n",
    "if p_value < CONFIDENCE_LEVEL:\n",
    "    printed.append(f\"SIGNIFICANT\")\n",
    "    significant_count += 1\n",
    "\n",
    "else:\n",
    "    non_significant_count += 1\n",
    "    printed.append(f\"NO\")\n",
    "\n",
    "\n",
    "fpath = \"result/analyze/5/number_of_test_cases_correlation_without_any_grouping.txt\"\n",
    "save_to_file(fpath, printed, significant_count, non_significant_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## group by dataset\n",
    "\n",
    "printed = []\n",
    "significant_count = 0\n",
    "non_significant_count = 0\n",
    "\n",
    "for dataset in datasets:\n",
    "    arr_1 = []\n",
    "    arr_2 = []\n",
    "    for asr in asrs:\n",
    "        for tool_short_name, tool in zip(tool_short_names, tools):\n",
    "          for seed in [\"1\", \"2\", \"3\"]:\n",
    "              arr_1 += number_of_test_cases[asr][dataset][tool][seed]\n",
    "          for metric in [\"WER_Seed1\", \"WER_Seed2\", \"WER_Seed3\"]:\n",
    "              arr_2 += relative_improvement_of_finetuned_model[asr][dataset][tool_short_name][metric]\n",
    "\n",
    "    assert len(arr_1) == len(arr_2)\n",
    "    correlation_coefficient, p_value = scipy.stats.spearmanr(arr_1, arr_2)\n",
    "\n",
    "    printed.append(f\"Dataset: {dataset}\")\n",
    "    printed.append(f\"Corr coef:\\t{correlation_coefficient}\")\n",
    "    printed.append(f\"p-value:\\t{p_value}\")\n",
    "    if p_value < CONFIDENCE_LEVEL:\n",
    "        printed.append(f\"SIGNIFICANT\")\n",
    "        significant_count += 1\n",
    "    else:\n",
    "        printed.append(f\"NO\")\n",
    "        non_significant_count += 1\n",
    "    printed.append(\"\")\n",
    "\n",
    "\n",
    "fpath = \"result/analyze/5/number_of_test_cases_correlation_group_by_dataset.txt\"\n",
    "save_to_file(fpath, printed, significant_count, non_significant_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## group by dataset and ASR\n",
    "\n",
    "printed = []\n",
    "significant_count = 0\n",
    "non_significant_count = 0\n",
    "\n",
    "\n",
    "for dataset in datasets:\n",
    "    for asr in asrs:\n",
    "        arr_1 = []\n",
    "        arr_2 = []\n",
    "        for tool_short_name, tool in zip(tool_short_names, tools):\n",
    "          for seed in [\"1\", \"2\", \"3\"]:\n",
    "              arr_1 += number_of_test_cases[asr][dataset][tool][seed]\n",
    "          for metric in [\"WER_Seed1\", \"WER_Seed2\", \"WER_Seed3\"]:\n",
    "              arr_2 += relative_improvement_of_finetuned_model[asr][dataset][tool_short_name][metric]\n",
    "\n",
    "        assert len(arr_1) == len(arr_2)\n",
    "        correlation_coefficient, p_value = scipy.stats.spearmanr(arr_1, arr_2)\n",
    "\n",
    "        printed.append(f\"Dataset: {dataset}\")\n",
    "        printed.append(f\"ASR: {asr}\")\n",
    "        printed.append(f\"Corr coef:\\t{correlation_coefficient}\")\n",
    "        printed.append(f\"p-value:\\t{p_value}\")\n",
    "        if p_value < CONFIDENCE_LEVEL:\n",
    "            printed.append(f\"SIGNIFICANT\")\n",
    "            significant_count += 1\n",
    "\n",
    "        else:\n",
    "            printed.append(f\"NO\")\n",
    "            non_significant_count += 1\n",
    "        printed.append(\"\")\n",
    "\n",
    "\n",
    "fpath = \"result/analyze/5/number_of_test_cases_correlation_group_by_dataset_and_asr.txt\"\n",
    "save_to_file(fpath, printed, significant_count, non_significant_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## group by dataset and ASR then averaging budget\n",
    "\n",
    "printed = []\n",
    "significant_count = 0\n",
    "non_significant_count = 0\n",
    "\n",
    "\n",
    "for dataset in datasets:\n",
    "    for asr in asrs:\n",
    "        arr_1 = []\n",
    "        arr_2 = []\n",
    "        for tool_short_name, tool in zip(tool_short_names, tools):\n",
    "            for seed in [\"1\", \"2\", \"3\"]:\n",
    "                curr_arr_1 = number_of_test_cases[asr][dataset][tool][seed]\n",
    "                arr_1.append(np.mean(curr_arr_1))\n",
    "            \n",
    "            for metric in [\"WER_Seed1\", \"WER_Seed2\", \"WER_Seed3\"]:\n",
    "                curr_arr_2 = relative_improvement_of_finetuned_model[asr][dataset][tool_short_name][metric]\n",
    "                arr_2.append(np.mean(curr_arr_2))\n",
    "\n",
    "\n",
    "        assert len(arr_1) == len(arr_2)\n",
    "        correlation_coefficient, p_value = scipy.stats.spearmanr(arr_1, arr_2)\n",
    "\n",
    "        printed.append(f\"Dataset: {dataset}\")\n",
    "        printed.append(f\"ASR: {asr}\")\n",
    "        printed.append(f\"Corr coef:\\t{correlation_coefficient}\")\n",
    "        printed.append(f\"p-value:\\t{p_value}\")\n",
    "        if p_value < CONFIDENCE_LEVEL:\n",
    "            printed.append(f\"SIGNIFICANT\")\n",
    "            significant_count += 1\n",
    "\n",
    "        else:\n",
    "            printed.append(f\"NO\")\n",
    "            non_significant_count += 1\n",
    "        printed.append(\"\")\n",
    "\n",
    "\n",
    "fpath = \"result/analyze/5/number_of_test_cases_correlation_group_by_dataset_and_asr_then_averaging_budget.txt\"\n",
    "save_to_file(fpath, printed, significant_count, non_significant_count)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
